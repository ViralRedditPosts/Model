{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name='AdministratorAccess', region_name='us-east-2')\n",
    "# resource vs client: https://www.learnaws.org/2021/02/24/boto3-resource-client/\n",
    "dynamodb_resource = session.resource('dynamodb')  #  higher level abstractions, recommended to use, fewer methods but creating table returns a table object that you can run operations on, can also grab a Table with Table('name')\n",
    "# dynamodb_client = session.client('dynamodb')  # low-level, more explicit methods. Creating table returns a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Rising Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risingTable = dynamodb_resource.Table('rising')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datesToQuery = utils.daysUntilNow()\n",
    "print(\"Dates to query:\", datesToQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postIdQueryResult = utils.queryByRangeOfDates(risingTable, datesToQuery)  # [{'postId': XXXXXX}, {'postId': YYYYYY}...]\n",
    "postsOfInterest = {res['postId'] for res in postIdQueryResult}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of posts found:\", len(postsOfInterest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "cfg_file = utils.findConfig()\n",
    "cfg = utils.parseConfig(cfg_file)\n",
    "\n",
    "spark = (\n",
    "  SparkSession\n",
    "  .builder\n",
    "  .appName('redditData')\n",
    "  .config(\"fs.s3a.access.key\", cfg['ACCESSKEY'])\n",
    "  .config(\"fs.s3a.secret.key\", cfg['SECRETKEY'])\n",
    "  .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can take a while due to read constraints placed on dynamo db, consider increasing RCU on database\n",
    "# it can also be slow because converts each dynamodb partition to a spark dataframe,\n",
    "# this was done so that it would scale better on a distributed system \n",
    "# over keeping all the data in python in one node and trying to then move it to spark\n",
    "postIdData = utils.getPostIdSparkDataFrame(spark, risingTable, postsOfInterest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasTestDf = postIdData.limit(5).toPandas()\n",
    "pandasTestDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "aggData = (\n",
    "  postIdData\n",
    "  .groupBy('postId', 'subreddit', 'title', 'createdTSUTC')\n",
    "  .agg(\n",
    "    F.max(F.when(F.col('timeElapsedMin') <= 20, F.col('score'))).alias('maxScore20m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(21,40), F.col('score'))).alias('maxScore21_40m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(41,60), F.col('score'))).alias('maxScore41_60m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin') <= 20, F.col('numComments'))).alias('maxNumComments20m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(21,40), F.col('numComments'))).alias('maxNumComments21_40m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(41,60), F.col('numComments'))).alias('maxNumComments41_60m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin') <= 20, F.col('upvoteRatio'))).alias('maxUpvoteRatio20m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(21,40), F.col('upvoteRatio'))).alias('maxUpvoteRatio21_40m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(41,60), F.col('upvoteRatio'))).alias('maxUpvoteRatio41_60m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin') <= 20, F.col('numGildings'))).alias('maxNumGildings20m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(21,40), F.col('numGildings'))).alias('maxNumGildings21_40m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(41,60), F.col('numGildings'))).alias('maxNumGildings41_60m')\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotTable = dynamodb_resource.Table('hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotPosts = utils.queryByRangeOfDates(hotTable, datesToQuery)\n",
    "uniqueHotPostIds = set([p['postId'] for p in hotPosts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hot posts are usually not a very long list and we really only need this for the purpose of creating targets\n",
    "print(\"unique hot postIds:\", uniqueHotPostIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def getTarget(postId:str, uniqueHotPostIds:set):\n",
    "  if postId in uniqueHotPostIds:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "  \n",
    "getTargetUDF = udf(lambda x: getTarget(x, uniqueHotPostIds), returnType=IntegerType())\n",
    "\n",
    "\n",
    "aggData = aggData.withColumn('target', getTargetUDF(F.col('postId')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by aggregating the data, there should be an at most 60x reduction in the data (since data can be collected once every minute)\n",
    "\n",
    "aggDataPd = aggData.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aggDataPd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggDataPd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggDataPd[aggDataPd['target']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pId in aggDataPd[aggDataPd['target']==1]['postId']:\n",
    "  print('https://reddit.com/'+pId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing, I've only collected about 1.5 days of data, with 7 viral posts (not a very large amount although that was to be expected). Interestingly, I've noticed that of the viral posts I have, \n",
    "\n",
    "- the one that had the most upvotes after an hour was actually the least viral, \n",
    "- while the one with the least upvotes was actually the most viral.\n",
    "- but that post with the least upvotes had the second most comments of the viral posts, 24 comments, so maybe it would be captured by the model\n",
    "\n",
    "I'm considering extending the time out to 90-120 minutes for data collection. However, the point was to get to a post early when there were relatively few comments. That most viral post had 24 comments after an hour and even that is a lot and any new replies are likely to be buried.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to S3\n",
    "\n",
    "This is basically our model data and what we will use to train a model. I used spark to write to s3 to future proof this if the data was too large to fit in pandas on driver.\n",
    "\n",
    "If you get an error here then you probably need to download hadoop-aws-*.jar (ex: [3.2.0](https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.2.0)) and aws-java-sdk-bundle-*.jar (ex: [1.11.375](https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-bundle/1.11.375))\n",
    "\n",
    "- for hadoop-aws-*.jar this should match the version of other hadoop jars in $SPARK_HOME/jars/\n",
    "- for aws-java-sdk-bundle-*.jar you will need to check the version dependency of hadoop-aws-*.jar on the maven website. Do NOT use the upgraded version, use the version that hadoop-aws was created with.\n",
    "\n",
    "You may not need to add these dependencies to the the configs, but you may need to restart the kernel and rerun things.\n",
    "\n",
    "These links may help:\n",
    "\n",
    "- [SO link 1](https://stackoverflow.com/questions/58415928/spark-s3-error-java-lang-classnotfoundexception-class-org-apache-hadoop-f?answertab=scoredesc#tab-top)\n",
    "- [SO link 2](https://stackoverflow.com/questions/44411493/java-lang-noclassdeffounderror-org-apache-hadoop-fs-storagestatistics/44500698#44500698)\n",
    "- [SO link 3](https://stackoverflow.com/questions/64547468/pyspark-s3-error-java-lang-noclassdeffounderror-com-amazonaws-amazonserviceex)\n",
    "- [Tutorial](https://notadatascientist.com/running-apache-spark-and-s3-locally/)\n",
    "- [Hadoop Troubleshooting](https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/troubleshooting_s3a.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggData.write.parquet(\"s3a://data-kennethmyers/redditAggregatedData.parquet\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
