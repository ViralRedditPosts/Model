{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name='AdministratorAccess', region_name='us-east-2')\n",
    "# resource vs client: https://www.learnaws.org/2021/02/24/boto3-resource-client/\n",
    "dynamodb_resource = session.resource('dynamodb')  #  higher level abstractions, recommended to use, fewer methods but creating table returns a table object that you can run operations on, can also grab a Table with Table('name')\n",
    "# dynamodb_client = session.client('dynamodb')  # low-level, more explicit methods. Creating table returns a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risingTable = dynamodb_resource.Table('rising')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def daysUntilNow():\n",
    "  now = datetime.utcnow().date()\n",
    "  return now\n",
    "daysUntilNow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb/table/query.html\n",
    "def queryByDate(date: str, projectionExpression: str = 'postId'):\n",
    "  return risingTable.query(\n",
    "    IndexName='byLoadDate',\n",
    "    KeyConditionExpression=Key('loadDateUTC').eq(date), \n",
    "    ProjectionExpression=projectionExpression\n",
    "  )['Items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenItems(listOfListOfItems):\n",
    "  return [item for sublist in listOfListOfItems for item in sublist] \n",
    "\n",
    "def queryByRangeOfDates(dates: list, projectionExpression: str = 'postId'):\n",
    "  returnedData = []\n",
    "  for d in dates:\n",
    "    returnedData.append(queryByDate(d, projectionExpression))\n",
    "  return flattenItems(returnedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postIdQueryResult = queryByRangeOfDates(['2023-04-09', '2023-04-10'])  # [{'postId': XXXXXX}, {'postId': YYYYYY}...]\n",
    "postsOfInterest = {res['postId'] for res in postIdQueryResult}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(postsOfInterest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from schema import fromDynamoConversion, toSparkSchema\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('redditData').getOrCreate()\n",
    "\n",
    "def applyDynamoConversions(dynamoRes, conversionFunctions=fromDynamoConversion):\n",
    "  return {k:fromDynamoConversion[k](v) for k,v in dynamoRes.items()}\n",
    "\n",
    "def getPostIdData(table, postId):\n",
    "  return table.query(\n",
    "    KeyConditionExpression=Key('postId').eq(postId), \n",
    "  )['Items']\n",
    "\n",
    "def getPostIdSparkDataFrame(table, postIds, flatten=True):\n",
    "  dataFrames = []  \n",
    "  for postId in postIds:\n",
    "    res = getPostIdData(table, postId)\n",
    "    res = [applyDynamoConversions(item) for item in res]\n",
    "    dataFrames.append(spark.createDataFrame(res, toSparkSchema))  # convert to DF\n",
    "  if flatten:\n",
    "    return reduce(DataFrame.unionAll, dataFrames)\n",
    "  else:\n",
    "    return returnedData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can take a while due to read constraints placed on dynamo db, consider increasing RCU on database\n",
    "# it can also be slow because converts each dynamodb partition to a spark dataframe,\n",
    "# this was done so that it would scale better on a distributed system \n",
    "# over keeping all the data in python in one node and trying to then move it to spark\n",
    "postIdData = getPostIdSparkDataFrame(risingTable, postsOfInterest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a little slow\n",
    "pandasTestDf = postIdData.limit(1000).toPandas()\n",
    "pandasTestDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "aggData = (\n",
    "  postIdData\n",
    "  .groupBy('postId', 'subreddit', 'title', 'createdTSUTC')\n",
    "  .agg(\n",
    "    F.max(F.when(F.col('timeElapsedMin') <= 20, F.col('score'))).alias('maxScore20m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(21,40), F.col('score'))).alias('maxScore21_40m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(41,60), F.col('score'))).alias('maxScore41_60m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin') <= 20, F.col('numComments'))).alias('maxNumComments20m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(21,40), F.col('numComments'))).alias('maxNumComments21_40m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(41,60), F.col('numComments'))).alias('maxNumComments41_60m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin') <= 20, F.col('upvoteRatio'))).alias('maxUpvoteRatio20m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(21,40), F.col('upvoteRatio'))).alias('maxUpvoteRatio21_40m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(41,60), F.col('upvoteRatio'))).alias('maxUpvoteRatio41_60m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin') <= 20, F.col('numGildings'))).alias('maxNumGildings20m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(21,40), F.col('numGildings'))).alias('maxNumGildings21_40m')\n",
    "    , F.max(F.when(F.col('timeElapsedMin').between(41,60), F.col('numGildings'))).alias('maxNumGildings41_60m')\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggDataPd = aggData.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggDataPd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
